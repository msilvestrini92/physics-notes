\section{Linear Maps}

We'll present linear maps with different definitions, one more practical, one geometric and finally the more abstract definition.

Practically, matrices are the coordinate version of linear maps. If you represent vectors as column, covectors as row vectors, linear maps are
represented by matrices.
How do matrices act on vectors? Let's see a simple example:

\begin{equation}
    \begingroup\color{red}\begin{bmatrix}
        3 & 4\\
        -1 & 2
    \end{bmatrix}\endgroup \begin{bmatrix}
        2\\
        1
    \end{bmatrix} = \begin{bmatrix}
        2\\
        0
    \end{bmatrix}
\end{equation}

What is actually doing? 
If we test using a "copy" of the basis vectors $\vec{e_1}, \vec{e_2}$, we see that the outputs are
respectively, the first and second column of the matrix:

\begin{equation}
    \begin{bmatrix}
        \begingroup\color{red}3\endgroup & 4\\
        \begingroup\color{red}-1\endgroup & 2
    \end{bmatrix} \begin{bmatrix}
        1\\
        0
    \end{bmatrix} = \begin{bmatrix}
        \begingroup\color{red}3\endgroup\\
        \begingroup\color{red}-1\endgroup
    \end{bmatrix}
\end{equation}

\begin{equation}
    \begin{bmatrix}
        3 & \begingroup\color{red}4\endgroup\\
        -1 & \begingroup\color{red}2\endgroup
    \end{bmatrix} \begin{bmatrix}
        0\\
        1
    \end{bmatrix} = \begin{bmatrix}
        \begingroup\color{red}4\endgroup\\
        \begingroup\color{red}2\endgroup
    \end{bmatrix}
\end{equation}

\textbf{Why do we stress about these being copies of basis vectors? Because Linear Maps tranform 
input vectors, they do not transform the basis.}
The $i^{th}$ column of the matrix is the image of the $i^{th}$ copy of the basis vector.

Geometrically, linear maps are spatial transforms that:
\begin{itemize}
    \item Keep gridlines parallel
    \item Keep gridlines evenly spaced
    \item Keep the origin stationary
\end{itemize}

And abstractly, linear maps are maps from a vector space to another:

\begin{equation}
    \textit{L}:\textit{V} \to \textit{W}
\end{equation}

And obey linearity rules:

\begin{align*}
    \textit{L}\left(\vec{v} + \vec{w}\right) &= \textit{L}\left(\vec{v}\right) + \textit{L}\left(\vec{w}\right)\\
    \textit{L}\left(n\vec{v}\right) &= n\textit{L}\left(\vec{v}\right)
\end{align*}

Now let's see how the abstract and coordinate definitions are related to each other. Let's review the formula for matrix multiplication of a 2x2 matrix:

\begin{equation}
    \begin{bmatrix}
        a & b\\
        c & d
    \end{bmatrix} \begin{bmatrix}
        \begingroup\color{red}x\endgroup\\
        \begingroup\color{blue}y\endgroup
    \end{bmatrix} = 
        \begin{bmatrix}
            a\begingroup\color{red}x\endgroup + b\begingroup\color{blue}y\endgroup\\
            c\begingroup\color{red}x\endgroup + d\begingroup\color{blue}y\endgroup
        \end{bmatrix}
\end{equation}

At a first glance, this rule may come out from nowhere, but this is actually a direct consequence of the abstract definition above.
To show this, let's start by saying that we have a linear map $\textit{L}:\textit{V} \to \textit{V}$ acting on a vector $\vec{v}$ to produce some output vector $\vec{w}$.
\textbf{The assumption to stay in the same vector space is needed to ensure we can use the same basis vectors on both inputs and outputs}:

\begin{align*}
    \begingroup\color{violet}\vec{w}\endgroup &= 
    \textit{L}\left(\begingroup\color{orange}\vec{v}\endgroup\right) = 
    \textit{L}\left(\begingroup\color{orange}v^1\endgroup\begingroup\color{blue}\vec{e_1}\endgroup + 
    \begingroup\color{orange}v^2\endgroup\begingroup\color{blue}\vec{e_2}\endgroup\right)\\
    &= \begingroup\color{orange}v^1\endgroup\textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right) + \begingroup\color{orange}v^2\endgroup\textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right)
\end{align*}

These $\textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right)$ and $\textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right)$ are
just vectors in $\textit{V}$ so we can write them down in terms of the basis vectors:

\begin{align*}
    \textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right) &= \textit{L}^1_1\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_1\begingroup\color{blue}\vec{e_2}\endgroup\\
    \textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right) &= \textit{L}^1_2\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_2\begingroup\color{blue}\vec{e_2}\endgroup
\end{align*}

\begin{align*}
    \begingroup\color{violet}\vec{w}\endgroup &= \begingroup\color{orange}v^1\endgroup\left(\textit{L}^1_1\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_1\begingroup\color{blue}\vec{e_2}\endgroup\right) + \begingroup\color{orange}v^2\endgroup\left(\textit{L}^1_2\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_2\begingroup\color{blue}\vec{e_2}\endgroup\right)\\
    &= \left(\textit{L}^1_1\begingroup\color{orange}v^1\endgroup + \textit{L}^1_2\begingroup\color{orange}v^2\endgroup\right) \begingroup\color{blue}\vec{e_1}\endgroup + \left(\textit{L}^2_1\begingroup\color{orange}v^1\endgroup + \textit{L}^2_2\begingroup\color{orange}v^2\endgroup\right) \begingroup\color{blue}\vec{e_2}\endgroup\\
    &= \begingroup\color{violet}w^1\endgroup\begingroup\color{blue}\vec{e_1}\endgroup + \begingroup\color{violet}w^2\endgroup\begingroup\color{blue}\vec{e_2}\endgroup
\end{align*}

So we've derived how to transform the $v$ coefficients into the $w$ coefficients.

\begin{align*}
    \begingroup\color{violet}w^1\endgroup &= \textit{L}^1_1\begingroup\color{orange}v^1\endgroup + \textit{L}^1_2\begingroup\color{orange}v^2\endgroup\\
    \begingroup\color{violet}w^2\endgroup &= \textit{L}^2_1\begingroup\color{orange}v^1\endgroup + \textit{L}^2_2\begingroup\color{orange}v^2\endgroup
\end{align*}

which are nothing more than the usual 2x2 matrix multiplication rule:

\begin{equation}
    \begin{bmatrix}
        L^1_1 & L^1_2\\
        L^2_1c & L^2_2
    \end{bmatrix} \begin{bmatrix}
        \begingroup\color{orange}v^1\endgroup\\
        \begingroup\color{orange}v^2\endgroup
    \end{bmatrix} = 
        \begin{bmatrix}
            \begingroup\color{violet}w^1\endgroup\\
            \begingroup\color{violet}w^2\endgroup
        \end{bmatrix}
\end{equation}

For any number of dimensions, we can write:

\begin{equation}
    \begingroup\color{violet}\vec{w}\endgroup = \textit{L}\left(\begingroup\color{orange}\vec{v}\endgroup\right) = \sum_{i=1}^{n}\begingroup\color{violet}w^i\endgroup\begingroup\color{blue}\vec{e_i}\endgroup
\end{equation}

And the linear maps coefficients:

\begin{equation}
    \textit{L}\left(\begingroup\color{blue}\vec{e_i}\endgroup\right) = \sum_{j=1}^{n}\textit{L}^j_i\begingroup\color{blue}\vec{e_j}\endgroup
\end{equation}

And we can transform finally, the $v$ components into $w$ components:

\begin{equation}
    \begingroup\color{violet}w^i\endgroup = \sum_{j=1}^{n}\textit{L}^i_j\begingroup\color{orange}v^j\endgroup
\end{equation}

which is indeed the matrix multiplication formula applied to n-dimension space.