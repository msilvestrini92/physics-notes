\section{Linear Maps}

We'll present linear maps with different definitions, one more practical, one geometric and finally the more abstract definition.

Practically, matrices are the coordinate version of linear maps. If you represent vectors as column, covectors as row vectors, linear maps are
represented by matrices.
How do matrices act on vectors? Let's see a simple example:

\begin{equation}
    \begingroup\color{red}\begin{bmatrix}
        3 & 4\\
        -1 & 2
    \end{bmatrix}\endgroup \begin{bmatrix}
        2\\
        1
    \end{bmatrix} = \begin{bmatrix}
        2\\
        0
    \end{bmatrix}
\end{equation}

What is actually doing? 
If we test using a "copy" of the basis vectors $\vec{e_1}, \vec{e_2}$, we see that the outputs are
respectively, the first and second column of the matrix:

\begin{equation}
    \begin{bmatrix}
        \begingroup\color{red}3\endgroup & 4\\
        \begingroup\color{red}-1\endgroup & 2
    \end{bmatrix} \begin{bmatrix}
        1\\
        0
    \end{bmatrix} = \begin{bmatrix}
        \begingroup\color{red}3\endgroup\\
        \begingroup\color{red}-1\endgroup
    \end{bmatrix}
\end{equation}

\begin{equation}
    \begin{bmatrix}
        3 & \begingroup\color{red}4\endgroup\\
        -1 & \begingroup\color{red}2\endgroup
    \end{bmatrix} \begin{bmatrix}
        0\\
        1
    \end{bmatrix} = \begin{bmatrix}
        \begingroup\color{red}4\endgroup\\
        \begingroup\color{red}2\endgroup
    \end{bmatrix}
\end{equation}

\textbf{Why do we stress about these being copies of basis vectors? Because Linear Maps tranform 
input vectors, they do not transform the basis.}
The $i^{th}$ column of the matrix is the image of the $i^{th}$ copy of the basis vector.

Geometrically, linear maps are spatial transforms that:
\begin{itemize}
    \item Keep gridlines parallel
    \item Keep gridlines evenly spaced
    \item Keep the origin stationary
\end{itemize}

And abstractly, linear maps are maps from a vector space to another:

\begin{equation}
    \textit{L}:\textit{V} \to \textit{W}
\end{equation}

And obey linearity rules:

\begin{align*}
    \textit{L}\left(\vec{v} + \vec{w}\right) &= \textit{L}\left(\vec{v}\right) + \textit{L}\left(\vec{w}\right)\\
    \textit{L}\left(n\vec{v}\right) &= n\textit{L}\left(\vec{v}\right)
\end{align*}

Now let's see how the abstract and coordinate definitions are related to each other. Let's review the formula for matrix multiplication of a 2x2 matrix:

\begin{equation}
    \begin{bmatrix}
        a & b\\
        c & d
    \end{bmatrix} \begin{bmatrix}
        \begingroup\color{red}x\endgroup\\
        \begingroup\color{blue}y\endgroup
    \end{bmatrix} = 
        \begin{bmatrix}
            a\begingroup\color{red}x\endgroup + b\begingroup\color{blue}y\endgroup\\
            c\begingroup\color{red}x\endgroup + d\begingroup\color{blue}y\endgroup
        \end{bmatrix}
\end{equation}

At a first glance, this rule may come out from nowhere, but this is actually a direct consequence of the abstract definition above.
To show this, let's start by saying that we have a linear map $\textit{L}:\textit{V} \to \textit{V}$ acting on a vector $\vec{v}$ to produce some output vector $\vec{w}$.
\textbf{The assumption to stay in the same vector space is needed to ensure we can use the same basis vectors on both inputs and outputs}:

\begin{align*}
    \begingroup\color{violet}\vec{w}\endgroup &= 
    \textit{L}\left(\begingroup\color{orange}\vec{v}\endgroup\right) = 
    \textit{L}\left(\begingroup\color{orange}v^1\endgroup\begingroup\color{blue}\vec{e_1}\endgroup + 
    \begingroup\color{orange}v^2\endgroup\begingroup\color{blue}\vec{e_2}\endgroup\right)\\
    &= \begingroup\color{orange}v^1\endgroup\textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right) + \begingroup\color{orange}v^2\endgroup\textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right)
\end{align*}

These $\textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right)$ and $\textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right)$ are
just vectors in $\textit{V}$ so we can write them down in terms of the basis vectors:

\begin{align*}
    \textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right) &= \textit{L}^1_1\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_1\begingroup\color{blue}\vec{e_2}\endgroup\\
    \textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right) &= \textit{L}^1_2\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_2\begingroup\color{blue}\vec{e_2}\endgroup
\end{align*}

\begin{align*}
    \begingroup\color{violet}\vec{w}\endgroup &= \begingroup\color{orange}v^1\endgroup\left(\textit{L}^1_1\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_1\begingroup\color{blue}\vec{e_2}\endgroup\right) + \begingroup\color{orange}v^2\endgroup\left(\textit{L}^1_2\begingroup\color{blue}\vec{e_1}\endgroup + \textit{L}^2_2\begingroup\color{blue}\vec{e_2}\endgroup\right)\\
    &= \left(\textit{L}^1_1\begingroup\color{orange}v^1\endgroup + \textit{L}^1_2\begingroup\color{orange}v^2\endgroup\right) \begingroup\color{blue}\vec{e_1}\endgroup + \left(\textit{L}^2_1\begingroup\color{orange}v^1\endgroup + \textit{L}^2_2\begingroup\color{orange}v^2\endgroup\right) \begingroup\color{blue}\vec{e_2}\endgroup\\
    &= \begingroup\color{violet}w^1\endgroup\begingroup\color{blue}\vec{e_1}\endgroup + \begingroup\color{violet}w^2\endgroup\begingroup\color{blue}\vec{e_2}\endgroup
\end{align*}

So we've derived how to transform the $v$ coefficients into the $w$ coefficients.

\begin{align*}
    \begingroup\color{violet}w^1\endgroup &= \textit{L}^1_1\begingroup\color{orange}v^1\endgroup + \textit{L}^1_2\begingroup\color{orange}v^2\endgroup\\
    \begingroup\color{violet}w^2\endgroup &= \textit{L}^2_1\begingroup\color{orange}v^1\endgroup + \textit{L}^2_2\begingroup\color{orange}v^2\endgroup
\end{align*}

which are nothing more than the usual 2x2 matrix multiplication rule:

\begin{equation}
    \begin{bmatrix}
        L^1_1 & L^1_2\\
        L^2_1c & L^2_2
    \end{bmatrix} \begin{bmatrix}
        \begingroup\color{orange}v^1\endgroup\\
        \begingroup\color{orange}v^2\endgroup
    \end{bmatrix} = 
        \begin{bmatrix}
            \begingroup\color{violet}w^1\endgroup\\
            \begingroup\color{violet}w^2\endgroup
        \end{bmatrix}
\end{equation}

For any number of dimensions, we can write:

\begin{equation}
    \begingroup\color{violet}\vec{w}\endgroup = \textit{L}\left(\begingroup\color{orange}\vec{v}\endgroup\right) = \sum_{i=1}^{n}\begingroup\color{violet}w^i\endgroup\begingroup\color{blue}\vec{e_i}\endgroup
\end{equation}

And the linear maps coefficients:

\begin{equation} \label{eq:linear-map-transform}
    \textit{L}\left(\begingroup\color{blue}\vec{e_i}\endgroup\right) = \sum_{j=1}^{n}\textit{L}^j_i\begingroup\color{blue}\vec{e_j}\endgroup
\end{equation}

And we can transform finally, the $v$ components into $w$ components:

\begin{equation}
    \begingroup\color{violet}w^i\endgroup = \sum_{j=1}^{n}\textit{L}^i_j\begingroup\color{orange}v^j\endgroup
\end{equation}

which is indeed the matrix multiplication formula applied to n-dimension space.

We can now proceed checking how linear maps transform when moving from one basis to another.

\begin{center}
\input{figures/05-linear-map-transform}
\end{center}

Consider the diagram above and the linear map expressed by the matrix in the $\color{blue}\vec{e_i}$ basis:

\begin{equation}
    \begin{bmatrix}
        1/2 & 0\\
        0 & 2
    \end{bmatrix}_{\color{blue}\vec{e_i}}
\end{equation}

As reminder, this means that:

\begin{align*}
    \textit{L}\left(\begingroup\color{blue}\vec{e_1}\endgroup\right) &= 1/2\begingroup\color{blue}\vec{e_1}\endgroup \\
    \textit{L}\left(\begingroup\color{blue}\vec{e_2}\endgroup\right) &= 2\begingroup\color{blue}\vec{e_2}\endgroup
\end{align*}

Vector $\color{orange}\vec{v}$ has components $1, 1$ in the basis $\color{blue}\vec{e_i}$.
So what would be the components of the vector $\textit{L}\left(\begingroup\color{orange}\vec{v}\endgroup\right)$ ?

\begin{equation}
    \textit{L}\left(
        \begin{bmatrix}
            1\\
            1
        \end{bmatrix}_{\color{blue}\vec{e_i}}
    \right) = 
    \begin{bmatrix}
        \begingroup\color{brown}1/2\endgroup\\
        \begingroup\color{brown}2\endgroup
    \end{bmatrix}_{\color{blue}\vec{e_i}}
\end{equation}

Remember that, if we apply the backward transformation to these vector components, we 
obtain the vector components in the new basis $\color{red}\tilde{\vec{e_i}}$:

\begin{equation}
    \begin{bmatrix}
        1/4 & 1/2\\
        -1 & 2
    \end{bmatrix}\begin{bmatrix}
        \begingroup\color{orange}1\endgroup\\
        \begingroup\color{orange}1\endgroup
    \end{bmatrix}_{\color{blue}\vec{e_i}} = 
    \begin{bmatrix}
        \begingroup\color{orange}3/4\endgroup\\
        \begingroup\color{orange}1\endgroup\\
    \end{bmatrix}_{\color{red}\tilde{\vec{e_i}}}
\end{equation}

So, the next question might be, what are the components of the linear map output vector
in the new basis?

\begin{equation}
    \textit{L}\left(
        \begin{bmatrix}
            \begingroup\color{orange}3/4\endgroup\\
            \begingroup\color{orange}1\endgroup\\
        \end{bmatrix}_{\color{red}\tilde{\vec{e_i}}}
    \right) = 
    \begin{bmatrix}
        \begingroup\color{brown}?\endgroup\\
        \begingroup\color{brown}?\endgroup
    \end{bmatrix}_{\color{red}\tilde{\vec{e_i}}}
\end{equation}

We cannot use the matrix representation we used before, because that's tied to the 
old basis vectors, so what we need to do is finding a new matrix that represents
the linear map in the new basis, which means we need to find these coefficients:

\begin{align*}
    \textit{L}\left(\begingroup\color{red}\tilde{\vec{e_1}}\endgroup\right) &= \tilde{L}^1_1\begingroup\color{red}\tilde{\vec{e_1}}\endgroup + \tilde{L}^2_1\begingroup\color{red}\tilde{\vec{e_2}}\endgroup\\
    \textit{L}\left(\begingroup\color{red}\tilde{\vec{e_2}}\endgroup\right) &= \tilde{L}^1_2\begingroup\color{red}\tilde{\vec{e_1}}\endgroup + \tilde{L}^2_2\begingroup\color{red}\tilde{\vec{e_2}}\endgroup
\end{align*}

\begin{equation}
    \sum_{q=1}^{n}\tilde{L}^q_i\begingroup\color{red}\tilde{\vec{e_q}}\endgroup = L\left(\begingroup\color{red}\tilde{\vec{e_i}}\endgroup\right)
\end{equation}

We can use the forward transform to re-write the new basis vectors in terms of the old ones:

\begin{align*}
    \sum_{q=1}^{n}\tilde{L}^q_i\begingroup\color{red}\tilde{\vec{e_q}}\endgroup &= L\left(\sum_{j=1}^{n} F^j_i \begingroup\color{blue}\vec{e_j}\endgroup\right)\\
    &= \sum_{j=1}^{n} F^j_i L\left(\begingroup\color{blue}\vec{e_j}\endgroup\right)
\end{align*}

Now we can use the formula that expresses the linear map output as linear combinaton of the old basis vectors (\ref{eq:linear-map-transform}) and re-arranging:

\begin{align*}
    \sum_{q=1}^{n}\tilde{L}^q_i\begingroup\color{red}\tilde{\vec{e_q}}\endgroup &= \sum_{j=1}^{n} F^j_i \sum_{k=1}^{n}\textit{L}^k_j\begingroup\color{blue}\vec{e_k}\endgroup\\
    &= \sum_{j=1}^{n}\sum_{k=1}^{n} F^j_i \textit{L}^k_j\begingroup\color{blue}\vec{e_k}\endgroup
\end{align*}

We can proceed re-writing the old basis vector in terms of the new one, using the backward transformation \ref{eq:backward}:

\begin{align*}
    \sum_{q=1}^{n}\tilde{L}^q_i\begingroup\color{red}\tilde{\vec{e_q}}\endgroup 
    &= \sum_{j=1}^{n}\sum_{k=1}^{n} F^j_i \textit{L}^k_j \sum_{l=1}^{n} B^l_k\begingroup\color{red}\tilde{\vec{e_l}}\endgroup\\
    &= \sum_{l=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n} B^l_k \textit{L}^k_j F^j_i \begingroup\color{red}\tilde{\vec{e_l}}\endgroup
\end{align*}

Since the letter we use for the summation does not really matter, we can just rename all the q to l and we get:

\begin{align*}
    \sum_{l=1}^{n}\tilde{L}^l_i\begingroup\color{red}\tilde{\vec{e_l}}\endgroup 
    &= \sum_{l=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n} B^l_k \textit{L}^k_j F^j_i \begingroup\color{red}\tilde{\vec{e_l}}\endgroup
\end{align*}

Finally, we see that the $\tilde{L}$ coefficients are obtained by multiplying the backward transform on the left, and forward transform on the right, of the old coefficients:

\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
    \tilde{L}^l_i = \sum_{j=1}^{n}\sum_{k=1}^{n}B^l_k L^k_j F^j_i
\end{empheq}
\end{subequations}

You can think about this, the $\widetilde{L}$ transformation takes us from the input vector components to the output vector components in the new basis.
Instead of "travelling" along this "arrow" (bottom of diagram below), we move from the new vector componens to the new vector components using the forward transformation.
Then, to transform the components of the input vector to the components of the output vector, we apply the $\textit{L}$ transformation.
And finally, to get from the old vector components to the new ones, we apply the backward transformation $B$

\begin{center}
\input{figures/05-linear-map-transformation-flow.tex}
\end{center}

Now you've noticed how heavy the derivation and notation, especially related to the 
summation involved, was above.
We will, from now on, adopt the Einstein summation convention, which states that, everytime indeces appear both at the bottom and at the top, a
summation is implicitly expected.

That means that in our case, of the linear map transformation:

\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
    \tilde{L}^l_i = B^l_k L^k_j F^j_i
\end{empheq}
\end{subequations}

Because the repeating indeces, appearing at the top and bottom, are k and j, they're implicitly summed from 1 to n.
Using this trick now, we can try deriving the backward transformation, from the $\widetilde{L}$ to the direct linear map $\textit{L}$.

\subsection*{Understanding Index Position in Einstein Notation}

Something you I was wondering is, when coming from conventional matrix multiplication, 
you clearly know that order or multiplication matters.
When we derived the above equation, you notice that at the end of the day, we re-arrange
coefficients to make it look ordered, but that really does not matter there, as all the components are scalar
quantities, as such, they obey the standard commutative property.

How to we relate these concepts then? It's all a matter of index position and contraction.\\
In tensor notation, the position of indices (upper vs. lower) carries crucial information about matrix multiplication:\\

\textbf{Convention for Linear Maps:}
For a linear map $L^i_j$:
\begin{itemize}
    \item \textbf{Lower index} $j$ = column index (input)
    \item \textbf{Upper index} $i$ = row index (output)
\end{itemize}

This comes directly from matrix-vector multiplication:
\begin{equation}
    w^i = L^i_j v^j
\end{equation}

The matrix "consumes" the $j$ index (contraction) and "produces" the $i$ index.\\
How do we read the transformation rule then?\\
For $\tilde{L}^l_i = B^l_k L^k_j F^j_i$, we trace the index flow:
\begin{center}
\begin{tabular}{rcl}
    $F^j_i$: & input $i$ (lower) & $\to$ output $j$ (upper)\\
    $L^k_j$: & input $j$ (lower) & $\to$ output $k$ (upper)\\
    $B^l_k$: & input $k$ (lower) & $\to$ output $l$ (upper)
\end{tabular}
\end{center}

The flow is $i \xrightarrow{F} j \xrightarrow{L} k \xrightarrow{B} l$, which tells us the matrix order is $\tilde{L} = B \cdot L \cdot F$.

\textbf{Key Point:} 
Repeated indices (one upper, one lower) indicate contractions (summations). The order of terms in Einstein
notation ($B^l_k L^k_j F^j_i$ vs $L^k_j F^j_i B^l_k$) doesn't matterâ€”it's the index connections that determine the matrix
multiplication order.\\ 

Let's take this equation, and what we want, is isolating the $L^k_j$ on the right side.
To do that, we need to take out those $B$ and $F$ maps on the right side.
How do we do this? We can consider that multiplying $F$ and $B$ together gives the identity matrix, so we can 
multiply by $F$ and $B$ on both sides of the equation.\\
But how do we choose the indeces? We need to follow a specific logic and rule: to contract with existing index, you need the opposit position (upper$\leftrightarrow$lower)
\begin{itemize}
    \item We start with $\tilde{L}^l_i = B^l_k L^k_j F^j_i$;
    \item We want to obtain $\textit{L}$ with two new indeces, let's say s and t: $\textit{L}^s_t$;
    \item We want to remove (i.e. contract) $B$ and $F$, which have free indeces l and i;
    \item Let's start with $B^l_k$, we have l to contract on the upper, so we choose to multiply by $F^s_l$;
    \item Now for $F^j_i$, we want i to contract on the lower, so we choose to multiply by $B^i_t$;
    \item At the end, these new coefficients give: $F^j_i B^i_t = \delta^j_t$ which is the identity matrix.
\end{itemize}

\begin{align*}
    \tilde{L}^l_i &= B^l_k L^k_j F^j_i\\
    \begingroup\color{red}F^s_l\endgroup \tilde{L}^l_i \begingroup\color{blue}B^i_t\endgroup &=
    \begingroup\color{red}F^s_l\endgroup B^l_k L^k_j F^j_i \begingroup\color{blue}B^i_t\endgroup\\
    \begingroup\color{red}F^s_l\endgroup \tilde{L}^l_i \begingroup\color{blue}B^i_t\endgroup &=
    \begingroup\color{red}\delta^{s}_k\endgroup L^k_j \begingroup\color{blue}\delta^{j}_t \endgroup\\
    \begingroup\color{red}F^s_l\endgroup \tilde{L}^l_i \begingroup\color{blue}B^i_t\endgroup &=
    \begingroup\color{red}\delta^{s}_k\endgroup L^k_t\\
    \begingroup\color{red}F^s_l\endgroup \tilde{L}^l_i \begingroup\color{blue}B^i_t\endgroup &=
    L^s_t
\end{align*}

So we obtain:

\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
    \begingroup\color{red}F^s_l\endgroup \tilde{L}^l_i \begingroup\color{blue}B^i_t\endgroup &=
    L^s_t
\end{empheq}
\end{subequations}

Now, summarizing this, we basically have found that linear maps are (1,1)-tensors, because
they transform using both forward and backward transformations, so they're both contravariant and covariant once each.

\subsection*{Exercises}

\begin{enumerate}
    \item \textbf{Basic Matrix Action}

    Given the linear map $L$ with matrix representation in basis $\{\vec{e_1}, \vec{e_2}\}$:
    \begin{equation*}
        L = \begin{bmatrix}
            2 & -1\\
            3 & 4
        \end{bmatrix}
    \end{equation*}
    \begin{enumerate}
        \item What is $L(\vec{e_1})$ in terms of $\vec{e_1}$ and $\vec{e_2}$?
        \item What is $L(\vec{e_2})$ in terms of $\vec{e_1}$ and $\vec{e_2}$?
        \item Compute $L(\vec{v})$ where $\vec{v} = 3\vec{e_1} - 2\vec{e_2}$.
    \end{enumerate}

    \item \textbf{Verifying Linearity}

    Which of the following maps are linear? For those that are not, explain why.
    \begin{enumerate}
        \item $T(\vec{v}) = 3\vec{v}$
        \item $T(x, y) = (x + 1, y)$
        \item $T(x, y) = (x + y, 2y)$
        \item $T(x, y) = (x^2, y)$
    \end{enumerate}

    \item \textbf{Constructing the Matrix}

    A linear map $L: \mathbb{R}^2 \to \mathbb{R}^2$ satisfies:
    \begin{align*}
        L(\vec{e_1}) &= 2\vec{e_1} + 3\vec{e_2}\\
        L(\vec{e_2}) &= -\vec{e_1} + 5\vec{e_2}
    \end{align*}
    Write down the matrix representation of $L$ and compute $L(4\vec{e_1} + 7\vec{e_2})$.

    \item \textbf{Einstein Notation Practice}

    Write out the following expressions explicitly without Einstein notation (showing all sums):
    \begin{enumerate}
        \item $v^i w_i$ (assuming 3 dimensions)
        \item $A^i_j B^j_k$ (for $2 \times 2$ matrices)
        \item $\delta^i_j v^j$
    \end{enumerate}

    \item \textbf{Index Manipulation}

    Starting from $w^i = L^i_j v^j$, multiply both sides by a covector $\alpha_i$ and simplify. What type of mathematical object is the final result?

    \item \textbf{Change of Basis - Forward and Backward}

    Consider two bases in $\mathbb{R}^2$: the standard basis $\{\vec{e_1}, \vec{e_2}\}$ and a new basis $\{\tilde{\vec{e_1}}, \tilde{\vec{e_2}}\}$ where:
    \begin{align*}
        \tilde{\vec{e_1}} &= 2\vec{e_1} + \vec{e_2}\\
        \tilde{\vec{e_2}} &= \vec{e_1} + 3\vec{e_2}
    \end{align*}
    \begin{enumerate}
        \item Write down the forward transformation matrix $F$.
        \item Compute the backward transformation matrix $B$ by finding $F^{-1}$.
        \item Verify that $FB = I$.
    \end{enumerate}

    \item \textbf{Linear Map Transformation}

    Using the bases from Exercise 6, consider a linear map with matrix representation:
    \begin{equation*}
        L = \begin{bmatrix}
            3 & 0\\
            0 & 1
        \end{bmatrix}_{\vec{e_i}}
    \end{equation*}
    in the original basis. Find the matrix representation $\tilde{L}$ in the new basis using the formula $\tilde{L}^l_i = B^l_k L^k_j F^j_i$.

    \item \textbf{Index Flow Analysis}

    For the expression $T^a_b = C^a_c D^c_d E^d_b$:
    \begin{enumerate}
        \item Identify all free indices and all dummy (summed) indices.
        \item Trace the index flow from input to output.
        \item Write the equivalent matrix multiplication.
    \end{enumerate}

    \item \textbf{Inverse Transformation}

    Given $\tilde{L}^l_i = B^l_k L^k_j F^j_i$, derive the expression for $L^k_j$ in terms of $\tilde{L}^l_i$ by:
    \begin{enumerate}
        \item Multiplying by appropriate forward/backward transforms
        \item Using the identity $F^s_l B^l_k = \delta^s_k$ and $B^i_t F^j_i = \delta^j_t$
        \item Simplifying using the Kronecker delta properties
    \end{enumerate}

    \item \textbf{Composition of Linear Maps}

    If ${L_1}^i_j$ and ${L_2}^j_k$ are two linear maps, their composition is $(L_1 \circ L_2)^i_k = {L_1}^i_j {L_2}^j_k$.
    \begin{enumerate}
        \item Explain why the index structure makes sense for composition.
        \item Show that if both $L_1$ and $L_2$ transform according to $\tilde{L} = BLF$, then their composition transforms correctly as well.
        \item Is the order of composition important? Verify by computing ${L_1}^i_j {L_2}^j_k$ vs ${L_2}^i_j {L_1}^j_k$ for:
        \begin{equation*}
            L_1 = \begin{bmatrix} 1 & 2\\ 0 & 1 \end{bmatrix}, \quad
            L_2 = \begin{bmatrix} 2 & 0\\ 1 & 3 \end{bmatrix}
        \end{equation*}
    \end{enumerate}
\end{enumerate}
